{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9c8e4b68-85ae-405e-99b6-bde9d4b74903",
      "metadata": {
        "id": "9c8e4b68-85ae-405e-99b6-bde9d4b74903"
      },
      "source": [
        "### ============================================================================\n",
        "# **REINFORCEMENT LEARNING ALGO AGENT on FX-EUR/USD FIN TRADING**\n",
        "###### 500k+ steps/hour\n",
        "###### Built on i5 3rd-gen/4GB DDR3\n",
        "### ============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f6df792-1241-419f-8df5-19dfc3c9cb91",
      "metadata": {
        "id": "8f6df792-1241-419f-8df5-19dfc3c9cb91"
      },
      "source": [
        "## 0.0 project setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36cf1b86-2ce1-411f-bfd8-8d38c0c51a26",
      "metadata": {
        "id": "36cf1b86-2ce1-411f-bfd8-8d38c0c51a26"
      },
      "outputs": [],
      "source": [
        "# for data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for data visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# for the RL-Algo Trading Agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# other utils\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d9686a-0b3c-415d-847e-b9801b8584d0",
      "metadata": {
        "id": "90d9686a-0b3c-415d-847e-b9801b8584d0"
      },
      "source": [
        "## 1.0 data handling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df_raw = pd.read_csv(\"/content/sample_data/eurusd_5yrs_daily.csv\")\n",
        "\n",
        "# split data train/test\n",
        "total_rows = len(df_raw)                                                         # 1304 rows\n",
        "train_size = int(0.7 * total_rows)                                               # 912 rows\n",
        "# test_size  = total_rows - train_size                                           # 392 rows\n",
        "train_raw = df_raw.iloc[:train_size].reset_index(drop=True)\n",
        "test_raw  = df_raw.iloc[train_size:].reset_index(drop=True)\n",
        "\n",
        "# Keep raw closes for PnL\n",
        "close_prices_train = train_raw['close'].values\n",
        "close_prices_test  = test_raw['close'].values\n",
        "\n",
        "# Scaling/Normalization\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_raw[['open', 'high', 'low', 'close']])\n",
        "train_scaled = pd.DataFrame(scaler.transform(train_raw[['open', 'high', 'low', 'close']]), columns=['open', 'high', 'low', 'close'])\n",
        "test_scaled  = pd.DataFrame(scaler.transform(test_raw[['open', 'high', 'low', 'close']]), columns=['open', 'high', 'low', 'close'])\n",
        "\n",
        "# Final datasets for training and testing\n",
        "df_train = train_scaled\n",
        "df_test  = test_scaled"
      ],
      "metadata": {
        "id": "gaQeHXlyGc2M"
      },
      "id": "gaQeHXlyGc2M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 environment design"
      ],
      "metadata": {
        "id": "2_x9tPM8uJ_T"
      },
      "id": "2_x9tPM8uJ_T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a State"
      ],
      "metadata": {
        "id": "bqc-km1nuP5t"
      },
      "id": "bqc-km1nuP5t"
    },
    {
      "cell_type": "code",
      "source": [
        "window = 30\n",
        "\n",
        "# util functions\n",
        "def compute_rsi(prices, period=14):\n",
        "    prices = pd.Series(prices)\n",
        "    delta = prices.diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "    roll_up = gain.ewm(com=period-1, min_periods=period).mean()\n",
        "    roll_down = loss.ewm(com=period-1, min_periods=period).mean()\n",
        "    rs = roll_up / (roll_down + 1e-8)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return float(rsi.iloc[-1]) if not rsi.empty else 50.0\n",
        "\n",
        "def compute_macd(prices, fast=12, slow=26):\n",
        "    prices = pd.Series(prices)\n",
        "    ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
        "    macd_line = ema_fast - ema_slow\n",
        "    return float(macd_line.iloc[-1])\n",
        "\n",
        "\n",
        "\n",
        "# create state\n",
        "def create_state(idx, df, raw_close_prices):\n",
        "\n",
        "    if idx < window:\n",
        "        return None\n",
        "\n",
        "    # log returns from scaled close-prices\n",
        "    scaled_close = df['close'].iloc[idx-window:idx].values\n",
        "    ratio = scaled_close[1:] / (scaled_close[:-1] + 1e-8)\n",
        "    ratio = np.clip(ratio, 0.9, 1.1)\n",
        "    returns = np.log(ratio)\n",
        "    returns = np.nan_to_num(returns, 0.0)\n",
        "\n",
        "    # some padding\n",
        "    padded_returns = np.zeros(window - 1)\n",
        "    padded_returns[-len(returns):] = returns\n",
        "\n",
        "    # features from raw close prices\n",
        "    raw_window = raw_close_prices[idx-window:idx]\n",
        "    raw_close = raw_window[-1]\n",
        "    rsi = compute_rsi(raw_window)\n",
        "    macd = compute_macd(raw_window)\n",
        "    ema20 = pd.Series(raw_window).ewm(span=20, adjust=False).mean().iloc[-1]\n",
        "    ema50 = pd.Series(raw_window).ewm(span=50, adjust=False).mean().iloc[-1]\n",
        "    ema20_dist = (raw_close - ema20) / raw_close\n",
        "    ema50_dist = (raw_close - ema50) / raw_close\n",
        "    vol = np.std(returns[-20:]) if len(returns) >= 5 else 0.0\n",
        "    momentum = returns[-1] if len(returns) > 0 else 0.0\n",
        "\n",
        "    # feature vector\n",
        "    features = np.array([rsi/100.0, macd, ema20_dist, ema50_dist, vol, momentum])\n",
        "\n",
        "    # state vector\n",
        "    state = np.concatenate([padded_returns, features]).astype(np.float32)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UVgRBQIMmesW"
      },
      "id": "UVgRBQIMmesW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 rl-algo architecture"
      ],
      "metadata": {
        "id": "v6uoXz_DnsRP"
      },
      "id": "v6uoXz_DnsRP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Neural Network"
      ],
      "metadata": {
        "id": "ZLpxL372mqbT"
      },
      "id": "ZLpxL372mqbT"
    },
    {
      "cell_type": "code",
      "source": [
        "class DuelingDDQN(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size=3, hidden=256):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Value stream\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden//2, 1)\n",
        "        )\n",
        "\n",
        "        # Advantage stream\n",
        "        self.advantage = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden//2, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        base = self.shared(x)\n",
        "        value = self.value(base)\n",
        "        advantage = self.advantage(base)\n",
        "        # Q = V + (A - mean(A))\n",
        "        q = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q"
      ],
      "metadata": {
        "id": "122tQ_aiuKuH"
      },
      "id": "122tQ_aiuKuH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buffer"
      ],
      "metadata": {
        "id": "fpnqE2HM2-AP"
      },
      "id": "fpnqE2HM2-AP"
    },
    {
      "cell_type": "code",
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "\n",
        "    def __init__(self, capacity=50_000, alpha=0.6, beta=0.4, beta_anneal=0.001):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.priorities = deque(maxlen=capacity)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.beta_anneal = beta_anneal\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        max_prio = max(self.priorities, default=1.0)\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "        self.priorities.append(max_prio ** self.alpha)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        prios = np.array(self.priorities) + 1e-6\n",
        "        probs = prios ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=False)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-self.beta)\n",
        "        weights /= weights.max()\n",
        "        self.beta = min(1.0, self.beta + self.beta_anneal)\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "        return (torch.FloatTensor(np.array(states)).to(device),\n",
        "                torch.LongTensor(actions).to(device),\n",
        "                torch.FloatTensor(rewards).to(device),\n",
        "                torch.FloatTensor(np.array(next_states)).to(device),\n",
        "                torch.FloatTensor(dones).to(device),\n",
        "                torch.FloatTensor(weights).to(device),\n",
        "                indices)\n",
        "\n",
        "    def update_priorities(self, indices, td_errors):\n",
        "        for i, error in zip(indices, td_errors):\n",
        "            self.priorities[i] = (error + 1e-6) ** self.alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "AfsHSev6nBiE"
      },
      "id": "AfsHSev6nBiE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "nvgf0a1NmfNN"
      },
      "id": "nvgf0a1NmfNN"
    },
    {
      "cell_type": "code",
      "source": [
        "# feed-in initializations\n",
        "state_size = window - 1 + 6\n",
        "action_size = 3\n",
        "\n",
        "# The Neural Nets Architecture\n",
        "policy_net = DuelingDDQN(state_size, action_size).to(device)\n",
        "target_net = DuelingDDQN(state_size, action_size).to(device)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.0005)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "buffer = PrioritizedReplayBuffer()\n",
        "target_update = 100\n",
        "\n",
        "# some training params\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "episodes = 50\n",
        "batch_size = 64\n",
        "all_rewards = []\n",
        "best_reward = -np.inf"
      ],
      "metadata": {
        "id": "4BD06ZJKQfGG"
      },
      "id": "4BD06ZJKQfGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(episodes):\n",
        "    ep_reward = 0.0\n",
        "    position = 0  # current position at start of bar\n",
        "\n",
        "    for idx in range(window, len(df_train)-1):\n",
        "        # ========================== CREATE STATE ==========================\n",
        "        state = create_state(idx, df_train, close_prices_train)\n",
        "        if state is None:\n",
        "            continue\n",
        "\n",
        "        # ========================== GET ACTION ==========================\n",
        "        if random.random() < epsilon:\n",
        "            action = random.randint(0, 2)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                action = policy_net(torch.FloatTensor(state).unsqueeze(0).to(device)).argmax().item()\n",
        "\n",
        "        # Decode action into new position\n",
        "        new_position = 1 if action == 1 else -1 if action == 2 else 0\n",
        "\n",
        "        # ========================== PRICE MOVE (THIS BAR) ==========================\n",
        "        raw_diff = close_prices_train[idx + 1] - close_prices_train[idx]\n",
        "        price_move_pips = raw_diff * 10000\n",
        "\n",
        "        # ========================== REWARD FROM CURRENT POSITION ==========================\n",
        "        reward = position * price_move_pips  # reward from holding through this bar\n",
        "\n",
        "        # ========================== REALISTIC TRANSACTION COSTS ==========================\n",
        "        if position != new_position:\n",
        "            if position == 0 or new_position == 0:\n",
        "                reward -= 1.0  # Flat <-> Position\n",
        "            else:\n",
        "                reward -= 2.0  # Reversal (Long <-> Short)\n",
        "\n",
        "        # ========================== OPTIONAL REWARD SHAPING (BONUSES/PENALTIES) =========\n",
        "        if abs(price_move_pips) > 25:\n",
        "            if (position == 1 and price_move_pips > 0) or (position == -1 and price_move_pips < 0):\n",
        "                reward += 18.0\n",
        "            elif position != 0:\n",
        "                reward -= 8.0\n",
        "\n",
        "        if position != 0:\n",
        "            if (position == 1 and price_move_pips > 0) or (position == -1 and price_move_pips < 0):\n",
        "                reward += 0.8\n",
        "            if (position == 1 and price_move_pips < -12) or (position == -1 and price_move_pips > 12):\n",
        "                reward -= 1.5\n",
        "            if abs(price_move_pips) > 8:\n",
        "                if (position == 1 and price_move_pips > 0) or (position == -1 and price_move_pips < 0):\n",
        "                    reward += 0.6\n",
        "\n",
        "        if position == 0 and abs(price_move_pips) < 15:\n",
        "            reward += 0.8\n",
        "\n",
        "        # ========================== UPDATE EPISODE REWARD & POSITION =======================\n",
        "        ep_reward += reward\n",
        "        position = new_position  # update AFTER reward\n",
        "\n",
        "        # ========================== NEXT STATE ==========================\n",
        "        next_state = create_state(idx + 1, df_train, close_prices_train)\n",
        "        if next_state is None:\n",
        "            next_state = state  # fallback\n",
        "\n",
        "        # ========================== STORE TRANSITION ==========================\n",
        "        buffer.push(state, action, reward, next_state, 0.0)\n",
        "\n",
        "        # ========================== TRAIN ==========================\n",
        "        if len(buffer) > 1000:\n",
        "            states, actions, rewards, next_states, dones, weights, indices = buffer.sample(batch_size)\n",
        "            current_q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "            with torch.no_grad():\n",
        "                next_actions = policy_net(next_states).argmax(1, keepdim=True)\n",
        "                next_q = target_net(next_states).gather(1, next_actions).squeeze(1)\n",
        "                target = rewards + gamma * next_q * (1 - dones)\n",
        "\n",
        "            loss = (weights * F.mse_loss(current_q, target, reduction='none')).mean()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            td_error = torch.abs(current_q - target).detach().cpu().numpy()\n",
        "            buffer.update_priorities(indices, td_error)\n",
        "\n",
        "    # ========================== EPISODE END ==========================\n",
        "    all_rewards.append(ep_reward)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    if (ep + 1) % 5 == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {ep+1:2d} | Reward: {ep_reward:8,.1f} pips | ε: {epsilon:.3f}\")\n",
        "\n",
        "    if ep_reward > best_reward:\n",
        "        best_reward = ep_reward\n",
        "        torch.save(policy_net.state_dict(), \"final_model.pth\")\n",
        "        print(f\"NEW BEST → {best_reward:,.0f} pips\")\n"
      ],
      "metadata": {
        "id": "DqsYWFOCw7wb"
      },
      "id": "DqsYWFOCw7wb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 evaluation"
      ],
      "metadata": {
        "id": "46Sibll8qLh8"
      },
      "id": "46Sibll8qLh8"
    },
    {
      "cell_type": "code",
      "source": [
        "# === FINAL REWARD PLOT ===\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(all_rewards, marker='o')\n",
        "plt.title(\"\\nDQN Training Progress (Pips)\\n\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward (pips)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n\\nBest DQN episode: {max(all_rewards):,.0f} pips\\n\")"
      ],
      "metadata": {
        "id": "Jig1Ua5vkZ_0"
      },
      "id": "Jig1Ua5vkZ_0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.0 backtesting"
      ],
      "metadata": {
        "id": "MshOFpZeBVtP"
      },
      "id": "MshOFpZeBVtP"
    },
    {
      "cell_type": "code",
      "source": [
        "model = DuelingDDQN(state_size=35, action_size=3).to(device)\n",
        "model.load_state_dict(torch.load('final_model.pth', map_location=device))\n",
        "model.eval()\n",
        "\n",
        "test_raw = pd.read_csv(\"/content/sample_data/eurusd_test_data.csv\")\n",
        "close_prices_test  = test_raw['close'].values\n",
        "df_test  = test_raw.copy()"
      ],
      "metadata": {
        "id": "j8D6jbyE64Qq"
      },
      "id": "j8D6jbyE64Qq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(df_data=None, name=\"CUSTOM\"):\n",
        "\n",
        "    df = df_test.copy()\n",
        "    close_prices = close_prices_test\n",
        "\n",
        "    position = 0\n",
        "    pnl = 0.0\n",
        "    trades = 0\n",
        "\n",
        "    for _ in range(window):\n",
        "        pass\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(window, len(df) - 1):\n",
        "\n",
        "            state = create_state(i, df, close_prices)\n",
        "            action = 0 if state is None else model(\n",
        "                torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            ).argmax().item()\n",
        "\n",
        "            new_position = 1 if action == 1 else -1 if action == 2 else 0\n",
        "\n",
        "            # --- Show new position immediately ---\n",
        "            current_position = new_position\n",
        "\n",
        "            # --- Realized PnL ---\n",
        "            pnl += position * (close_prices[i] - close_prices[i - 1]) * 10000\n",
        "\n",
        "            # --- Transaction costs ---\n",
        "            if position != new_position:\n",
        "                if position == 0 or new_position == 0:\n",
        "                    pnl -= 1.0\n",
        "                    trades += 1\n",
        "                else:\n",
        "                    pnl -= 2.0\n",
        "                    trades += 2\n",
        "\n",
        "            position = new_position\n",
        "\n",
        "    return round(pnl, 2)\n"
      ],
      "metadata": {
        "id": "4mEUaJW3BVT7"
      },
      "id": "4mEUaJW3BVT7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}